{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2db13900",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f44067f",
   "metadata": {},
   "source": [
    "# Table of Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1d0033",
   "metadata": {},
   "source": [
    "## 1. Multi-Head Connection\n",
    "\n",
    "The following is the architecutre MultiHeadAttention layer, we can use it as reference when implementing the multi-head layer:\n",
    "\n",
    "![multihead_attention](../images/multi-head_attention.png)\n",
    "\n",
    "In simple english this is what is invovled in the multi-head layer:\n",
    "\n",
    "1. Configure the number of heads you need (hyper-parameter) \n",
    "2. Inputs to MH layer are 3 word vectors(Query, Key, Value) and it outputs a context aware vector \n",
    "3. The inputs are passed to each attention head which have 3 Dense Layers (learnable)\n",
    "4. Finally the outputs of each head is concatenated and output is presented\n",
    "\n",
    "### Keras has an implementation of multi-head layer:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a1c358e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1\n",
      "\n",
      "systemMemory: 8.00 GB\n",
      "maxCacheSize: 2.67 GB\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-07 19:35:00.508719: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2022-01-07 19:35:00.508823: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "num_heads = 2\n",
    "embedding_vector_dim = 256\n",
    "inputs = tf.keras.Input(shape=[8, 256])\n",
    "mha_layer = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embedding_vector_dim)\n",
    "outputs = mha_layer(inputs, inputs, inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12492fa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 8, 256)\n"
     ]
    }
   ],
   "source": [
    "print(outputs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471dcd72",
   "metadata": {},
   "source": [
    "## 2. Transformer Encoder\n",
    "\n",
    "The following is the architecture of Encoder from the original \"Attention is all you need\"paper\n",
    "\n",
    "![transformer_encoder](../images/transformer_encoder.png)\n",
    "\n",
    "In breif this is what is invovled in the Transformer Encoder layer:\n",
    "\n",
    "1. It begins with a multihead attention (as described above)\n",
    "2. The original word vectors have a residual connection with the output from multihead attention\n",
    "3. Then the output goes through a Normalization layer, NL1\n",
    "4. Now we have a dense projection block (2 Dense layers maybe configurable)\n",
    "   output of this layer is equal to the output/input vector dimension\n",
    "5. Then we have a residual connection of the NL1 with the output of Dense projection\n",
    "6. Finally we have one more Normalization layer, NL2\n",
    "\n",
    "### A quick note on why we use residual connection and Normalization:\n",
    "\n",
    "1.Why residual Connection?\n",
    " - It is a fix against vanishing gradient problem\n",
    " - It acts as a information shortcut around destructive or nosiy blocks such as blocks that contain relu activations or dropout layers)\n",
    " - It enables the gradient info to flow noiselessy propogate in a Deep Network\n",
    " \n",
    " 2.Why use normalization layer?\n",
    " - It helps in graidents flow better during backprop\n",
    " - The Normalization we use here is LayerNormalization layer, which normalizes each sequence independently from other sequences in the batch.\n",
    " Note: BatchNorm doesn't work that great with sequence data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36425c91",
   "metadata": {},
   "source": [
    "## 3. The Code for Tranformer Encoder layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9023364e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "131b6aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "        self.embed_dim = embed_dim #Vector embedding Dimension\n",
    "        self.dense_dim = dense_dim #Dense layers number of neurons\n",
    "        self.num_heads = num_heads #Number of heads in your MLH Layer\n",
    "        self.attention = tf.keras.layers.MultiHeadAttention( #implementing the mutlihead attention block\n",
    "                         num_heads = num_heads,\n",
    "                         key_dim   = embed_dim )\n",
    "        self.dnse_proj = tf.keras.Sequential(\n",
    "                         [tf.keras.layers.Dense(dense_dim, activation='relu'),\n",
    "                          tf.keras.layers.Dense(embed_dim)\n",
    "                         ]\n",
    "                         )\n",
    "        self.layrnorm1 = tf.keras.layers.LayerNormalization()\n",
    "        self.layrnorm2 = tf.keras.layers.LayerNormalization()\n",
    "    \n",
    "    def call(self, inputs, mask=None):\n",
    "        if mask is not None:\n",
    "            mask = mask[:, tf.newaxis, :]\n",
    "        \n",
    "        attention_output = self.attention(inputs, inputs,\n",
    "                                          attention_mask=mask)\n",
    "        #Input to projection layer\n",
    "        proj_input = self.layrnorm1(inputs + attention_output)\n",
    "        \n",
    "        #Dense block computation\n",
    "        proj_output = self.dnse_proj(proj_input)\n",
    "        \n",
    "        #Finally add the Dense projection output with along with its original input passed to it\n",
    "        return self.layrnorm2(proj_input + proj_output)\n",
    "    \n",
    "    def get_config(self):\n",
    "        \n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"embed_dim\": self.embed_dim,\n",
    "            \"dense_dim\": self.dense_dim,\n",
    "            \"num_heads\": self.num_heads,\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d57696b7",
   "metadata": {},
   "source": [
    "## Now we are going to build a text classifier using the Transformer Encoder Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c84f39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as  np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b1fcd2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/uhack_review_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc05d49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = df['Review']\n",
    "lables = df['Polarity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4bcc9a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## splitting data\n",
    "train_size = int(0.9 * len(df))\n",
    "train_data = df[:train_size]\n",
    "test_data = df[train_size:]\n",
    "\n",
    "\n",
    "train_sentences = train_data['Review'].values\n",
    "test_sentences = test_data['Review'].values\n",
    "\n",
    "train_labels = np.array(train_data['Polarity'].values)\n",
    "test_labels = np.array(test_data['Polarity'].values)\n",
    "\n",
    "\n",
    "## HYPER-PARAM:-\n",
    "\n",
    "NUM_WORDS = 1000\n",
    "TRUNCATE = 'post'  # 'pre'\n",
    "PADDING = 'post'   # 'pre\n",
    "MAX_LEN = 100\n",
    "EVD = 16\n",
    "\n",
    "## 1. Fit Tokenizer\n",
    "\n",
    "bbc_tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=NUM_WORDS,\n",
    "                                                      oov_token='<OOV>')\n",
    "bbc_tokenizer.fit_on_texts(train_sentences)\n",
    "\n",
    "## 2. Convert text to sequence\n",
    "\n",
    "train_seq = bbc_tokenizer.texts_to_sequences(train_sentences)\n",
    "test_seq = bbc_tokenizer.texts_to_sequences(test_sentences)\n",
    "\n",
    "## 3. Convert the sequence to padded sequences\n",
    "\n",
    "train_padded = tf.keras.preprocessing.sequence.pad_sequences(train_seq,\n",
    "                                                             truncating=TRUNCATE,\n",
    "                                                             padding=PADDING,\n",
    "                                                             maxlen=MAX_LEN)\n",
    "test_padded = tf.keras.preprocessing.sequence.pad_sequences(test_seq,\n",
    "                                                             truncating=TRUNCATE,\n",
    "                                                             padding=PADDING,\n",
    "                                                             maxlen=MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b5694b8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         [(None, None)]            0         \n",
      "_________________________________________________________________\n",
      "embedding_2 (Embedding)      (None, None, 16)          16000     \n",
      "_________________________________________________________________\n",
      "transformer_encoder_2 (Trans (None, None, 16)          3296      \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d (Global (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 19,313\n",
      "Trainable params: 19,313\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "## Classification\n",
    "\n",
    "## Hyper-params for transformer\n",
    "\n",
    "num_heads = 2\n",
    "dense_dim = 32\n",
    "\n",
    "\n",
    "inputs = tf.keras.Input(shape=[None], dtype='int64')\n",
    "embedd = tf.keras.layers.Embedding(NUM_WORDS,EVD)(inputs)\n",
    "transf = TransformerEncoder(EVD, dense_dim, num_heads)(embedd)\n",
    "glmaxp = tf.keras.layers.GlobalMaxPool1D()(transf)\n",
    "droput = tf.keras.layers.Dropout(0.5)(glmaxp)\n",
    "output = tf.keras.layers.Dense(1, activation='sigmoid')(droput)\n",
    "\n",
    "tmodel = tf.keras.models.Model(inputs, output)\n",
    "\n",
    "tmodel.compile(optimizer='adam',\n",
    "               loss='binary_crossentropy',\n",
    "               metrics=['accuracy'])\n",
    "tmodel.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9d95e516",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-07 19:44:11.345843: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n",
      "2022-01-07 19:44:11.345877: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n",
      "2022-01-07 19:44:11.347027: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n",
      "2022-01-07 19:44:11.466005: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "2022-01-07 19:44:11.468526: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-07 19:44:11.750198: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  5/173 [..............................] - ETA: 7s - loss: 1.0123 - accuracy: 0.5437"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-07 19:44:14.939784: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n",
      "2022-01-07 19:44:14.939798: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n",
      "2022-01-07 19:44:14.992051: I tensorflow/core/profiler/lib/profiler_session.cc:66] Profiler session collecting data.\n",
      "2022-01-07 19:44:14.995497: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n",
      "2022-01-07 19:44:14.999798: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: ../tboard/train/plugins/profile/2022_01_07_19_44_14\n",
      "\n",
      "2022-01-07 19:44:15.001006: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for trace.json.gz to ../tboard/train/plugins/profile/2022_01_07_19_44_14/Virajdatts-MacBook-Air.local.trace.json.gz\n",
      "2022-01-07 19:44:15.004255: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: ../tboard/train/plugins/profile/2022_01_07_19_44_14\n",
      "\n",
      "2022-01-07 19:44:15.004491: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for memory_profile.json.gz to ../tboard/train/plugins/profile/2022_01_07_19_44_14/Virajdatts-MacBook-Air.local.memory_profile.json.gz\n",
      "2022-01-07 19:44:15.004873: I tensorflow/core/profiler/rpc/client/capture_profile.cc:251] Creating directory: ../tboard/train/plugins/profile/2022_01_07_19_44_14\n",
      "Dumped tool data for xplane.pb to ../tboard/train/plugins/profile/2022_01_07_19_44_14/Virajdatts-MacBook-Air.local.xplane.pb\n",
      "Dumped tool data for overview_page.pb to ../tboard/train/plugins/profile/2022_01_07_19_44_14/Virajdatts-MacBook-Air.local.overview_page.pb\n",
      "Dumped tool data for input_pipeline.pb to ../tboard/train/plugins/profile/2022_01_07_19_44_14/Virajdatts-MacBook-Air.local.input_pipeline.pb\n",
      "Dumped tool data for tensorflow_stats.pb to ../tboard/train/plugins/profile/2022_01_07_19_44_14/Virajdatts-MacBook-Air.local.tensorflow_stats.pb\n",
      "Dumped tool data for kernel_stats.pb to ../tboard/train/plugins/profile/2022_01_07_19_44_14/Virajdatts-MacBook-Air.local.kernel_stats.pb\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "173/173 [==============================] - ETA: 0s - loss: 0.5057 - accuracy: 0.7677"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-07 19:44:20.748037: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "173/173 [==============================] - 10s 36ms/step - loss: 0.5057 - accuracy: 0.7677 - val_loss: 0.3550 - val_accuracy: 0.8583\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.35500, saving model to ../model/first_transformer.h5\n",
      "Epoch 2/10\n",
      "173/173 [==============================] - 6s 33ms/step - loss: 0.3638 - accuracy: 0.8327 - val_loss: 0.3082 - val_accuracy: 0.8730\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.35500 to 0.30816, saving model to ../model/first_transformer.h5\n",
      "Epoch 3/10\n",
      "173/173 [==============================] - 6s 32ms/step - loss: 0.3154 - accuracy: 0.8647 - val_loss: 0.2890 - val_accuracy: 0.8860\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.30816 to 0.28899, saving model to ../model/first_transformer.h5\n",
      "Epoch 4/10\n",
      "173/173 [==============================] - 6s 33ms/step - loss: 0.2879 - accuracy: 0.8816 - val_loss: 0.2809 - val_accuracy: 0.8844\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.28899 to 0.28093, saving model to ../model/first_transformer.h5\n",
      "Epoch 5/10\n",
      "173/173 [==============================] - 6s 32ms/step - loss: 0.2760 - accuracy: 0.8845 - val_loss: 0.2729 - val_accuracy: 0.8990\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.28093 to 0.27290, saving model to ../model/first_transformer.h5\n",
      "Epoch 6/10\n",
      "173/173 [==============================] - 6s 33ms/step - loss: 0.2593 - accuracy: 0.8883 - val_loss: 0.2692 - val_accuracy: 0.8958\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.27290 to 0.26919, saving model to ../model/first_transformer.h5\n",
      "Epoch 7/10\n",
      "173/173 [==============================] - 6s 33ms/step - loss: 0.2542 - accuracy: 0.8904 - val_loss: 0.2684 - val_accuracy: 0.8958\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.26919 to 0.26844, saving model to ../model/first_transformer.h5\n",
      "Epoch 8/10\n",
      "173/173 [==============================] - 6s 34ms/step - loss: 0.2431 - accuracy: 0.8989 - val_loss: 0.2608 - val_accuracy: 0.8990\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.26844 to 0.26077, saving model to ../model/first_transformer.h5\n",
      "Epoch 9/10\n",
      "173/173 [==============================] - 6s 33ms/step - loss: 0.2295 - accuracy: 0.9053 - val_loss: 0.2565 - val_accuracy: 0.8974\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.26077 to 0.25654, saving model to ../model/first_transformer.h5\n",
      "Epoch 10/10\n",
      "173/173 [==============================] - 6s 33ms/step - loss: 0.2091 - accuracy: 0.9162 - val_loss: 0.2568 - val_accuracy: 0.9023\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.25654\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x168fd7760>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MC = tf.keras.callbacks.ModelCheckpoint(\n",
    "    '../model/first_transformer.h5',\n",
    "    monitor='val_loss',\n",
    "    save_best_only='True',\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "ES = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=5,\n",
    "    verbose=1,\n",
    "    restore_best_weights='True'\n",
    ")\n",
    "\n",
    "TB = tf.keras.callbacks.TensorBoard('../tboard/')\n",
    "\n",
    "tmodel.fit(train_padded,\n",
    "              train_labels,\n",
    "               epochs=10,\n",
    "               validation_data=(test_padded, test_labels),\n",
    "               callbacks=[ES, MC, TB]\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40ad0b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('base': conda)",
   "language": "python",
   "name": "python397jvsc74a57bd09856851bd649d589f665b7dab82e57037201f90d2f14c285519476a7544379c3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
