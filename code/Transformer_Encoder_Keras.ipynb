{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2db13900",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f44067f",
   "metadata": {},
   "source": [
    "# Table of Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1d0033",
   "metadata": {},
   "source": [
    "## 1. Multi-Head Connection\n",
    "\n",
    "The following is the architecutre MultiHeadAttention layer, we can use it as reference when implementing the multi-head layer:\n",
    "\n",
    "![multihead_attention](../images/multi-head_attention.png)\n",
    "\n",
    "In simple english this is what is invovled in the multi-head layer:\n",
    "\n",
    "1. Configure the number of heads you need (hyper-parameter) \n",
    "2. Inputs to MH layer are 3 word vectors(Query, Key, Value) and it outputs a context aware vector \n",
    "3. The inputs are passed to each attention head which have 3 Dense Layers (learnable)\n",
    "4. Finally the outputs of each head is concatenated and output is presented\n",
    "\n",
    "### Keras has an implementation of multi-head layer:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a1c358e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1\n",
      "\n",
      "systemMemory: 8.00 GB\n",
      "maxCacheSize: 2.67 GB\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-07 21:06:57.998932: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2022-01-07 21:06:57.999543: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "num_heads = 2\n",
    "embedding_vector_dim = 256\n",
    "inputs = tf.keras.Input(shape=[8, 256])\n",
    "mha_layer = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embedding_vector_dim)\n",
    "outputs = mha_layer(inputs, inputs, inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12492fa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 8, 256)\n"
     ]
    }
   ],
   "source": [
    "print(outputs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471dcd72",
   "metadata": {},
   "source": [
    "## 2. Transformer Encoder\n",
    "\n",
    "The following is the architecture of Encoder from the original \"Attention is all you need\"paper\n",
    "\n",
    "![transformer_encoder](../images/transformer_encoder.png)\n",
    "\n",
    "In breif this is what is invovled in the Transformer Encoder layer:\n",
    "\n",
    "1. It begins with a multihead attention (as described above)\n",
    "2. The original word vectors have a residual connection with the output from multihead attention\n",
    "3. Then the output goes through a Normalization layer, NL1\n",
    "4. Now we have a dense projection block (2 Dense layers maybe configurable)\n",
    "   output of this layer is equal to the output/input vector dimension\n",
    "5. Then we have a residual connection of the NL1 with the output of Dense projection\n",
    "6. Finally we have one more Normalization layer, NL2\n",
    "\n",
    "### A quick note on why we use residual connection and Normalization:\n",
    "\n",
    "1.Why residual Connection?\n",
    " - It is a fix against vanishing gradient problem\n",
    " - It acts as a information shortcut around destructive or nosiy blocks such as blocks that contain relu activations or dropout layers)\n",
    " - It enables the gradient info to flow noiselessy propogate in a Deep Network\n",
    " \n",
    " 2.Why use normalization layer?\n",
    " - It helps in graidents flow better during backprop\n",
    " - The Normalization we use here is LayerNormalization layer, which normalizes each sequence independently from other sequences in the batch.\n",
    " Note: BatchNorm doesn't work that great with sequence data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36425c91",
   "metadata": {},
   "source": [
    "## 3. The Code for Tranformer Encoder layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9023364e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "131b6aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "        self.embed_dim = embed_dim #Vector embedding Dimension\n",
    "        self.dense_dim = dense_dim #Dense layers number of neurons\n",
    "        self.num_heads = num_heads #Number of heads in your MLH Layer\n",
    "        self.attention = tf.keras.layers.MultiHeadAttention( #implementing the mutlihead attention block\n",
    "                         num_heads = num_heads,\n",
    "                         key_dim   = embed_dim )\n",
    "        self.dnse_proj = tf.keras.Sequential(\n",
    "                         [tf.keras.layers.Dense(dense_dim, activation='relu'),\n",
    "                          tf.keras.layers.Dense(embed_dim)\n",
    "                         ]\n",
    "                         )\n",
    "        self.layrnorm1 = tf.keras.layers.LayerNormalization()\n",
    "        self.layrnorm2 = tf.keras.layers.LayerNormalization()\n",
    "    \n",
    "    def call(self, inputs, mask=None):\n",
    "        if mask is not None:\n",
    "            mask = mask[:, tf.newaxis, :]\n",
    "        \n",
    "        attention_output = self.attention(inputs, inputs,\n",
    "                                          attention_mask=mask)\n",
    "        #Input to projection layer\n",
    "        proj_input = self.layrnorm1(inputs + attention_output)\n",
    "        \n",
    "        #Dense block computation\n",
    "        proj_output = self.dnse_proj(proj_input)\n",
    "        \n",
    "        #Finally add the Dense projection output with along with its original input passed to it\n",
    "        return self.layrnorm2(proj_input + proj_output)\n",
    "    \n",
    "    def get_config(self):\n",
    "        \n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"embed_dim\": self.embed_dim,\n",
    "            \"dense_dim\": self.dense_dim,\n",
    "            \"num_heads\": self.num_heads,\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d57696b7",
   "metadata": {},
   "source": [
    "## Now we are going to build a text classifier using the Transformer Encoder Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c84f39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as  np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b1fcd2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/uhack_review_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc05d49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = df['Review']\n",
    "lables = df['Polarity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4bcc9a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## splitting data\n",
    "train_size = int(0.9 * len(df))\n",
    "train_data = df[:train_size]\n",
    "test_data = df[train_size:]\n",
    "\n",
    "\n",
    "train_sentences = train_data['Review'].values\n",
    "test_sentences = test_data['Review'].values\n",
    "\n",
    "train_labels = np.array(train_data['Polarity'].values)\n",
    "test_labels = np.array(test_data['Polarity'].values)\n",
    "\n",
    "\n",
    "## HYPER-PARAM:-\n",
    "\n",
    "NUM_WORDS = 1000\n",
    "TRUNCATE = 'post'  # 'pre'\n",
    "PADDING = 'post'   # 'pre\n",
    "MAX_LEN = 100\n",
    "EVD = 16\n",
    "\n",
    "## 1. Fit Tokenizer\n",
    "\n",
    "bbc_tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=NUM_WORDS,\n",
    "                                                      oov_token='<OOV>')\n",
    "bbc_tokenizer.fit_on_texts(train_sentences)\n",
    "\n",
    "## 2. Convert text to sequence\n",
    "\n",
    "train_seq = bbc_tokenizer.texts_to_sequences(train_sentences)\n",
    "test_seq = bbc_tokenizer.texts_to_sequences(test_sentences)\n",
    "\n",
    "## 3. Convert the sequence to padded sequences\n",
    "\n",
    "train_padded = tf.keras.preprocessing.sequence.pad_sequences(train_seq,\n",
    "                                                             truncating=TRUNCATE,\n",
    "                                                             padding=PADDING,\n",
    "                                                             maxlen=MAX_LEN)\n",
    "test_padded = tf.keras.preprocessing.sequence.pad_sequences(test_seq,\n",
    "                                                             truncating=TRUNCATE,\n",
    "                                                             padding=PADDING,\n",
    "                                                             maxlen=MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b5694b8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, None)]            0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, None, 16)          16000     \n",
      "_________________________________________________________________\n",
      "transformer_encoder (Transfo (None, None, 16)          3296      \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d (Global (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 19,313\n",
      "Trainable params: 19,313\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "## Classification\n",
    "\n",
    "## Hyper-params for transformer\n",
    "\n",
    "num_heads = 2\n",
    "dense_dim = 32\n",
    "\n",
    "\n",
    "inputs = tf.keras.Input(shape=[None], dtype='int64')\n",
    "embedd = tf.keras.layers.Embedding(NUM_WORDS,EVD)(inputs)\n",
    "transf = TransformerEncoder(EVD, dense_dim, num_heads)(embedd)\n",
    "glmaxp = tf.keras.layers.GlobalMaxPool1D()(transf)\n",
    "droput = tf.keras.layers.Dropout(0.5)(glmaxp)\n",
    "output = tf.keras.layers.Dense(1, activation='sigmoid')(droput)\n",
    "\n",
    "tmodel = tf.keras.models.Model(inputs, output)\n",
    "\n",
    "tmodel.compile(optimizer='adam',\n",
    "               loss='binary_crossentropy',\n",
    "               metrics=['accuracy'])\n",
    "tmodel.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9d95e516",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-07 21:06:58.609593: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n",
      "2022-01-07 21:06:58.609601: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n",
      "2022-01-07 21:06:58.609735: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n",
      "2022-01-07 21:06:58.649818: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "2022-01-07 21:06:58.652122: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "2022-01-07 21:06:58.927614: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  5/173 [..............................] - ETA: 7s - loss: 1.7985 - accuracy: 0.3500"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-07 21:07:00.473874: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n",
      "2022-01-07 21:07:00.473885: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n",
      "2022-01-07 21:07:00.512749: I tensorflow/core/profiler/lib/profiler_session.cc:66] Profiler session collecting data.\n",
      "2022-01-07 21:07:00.516370: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n",
      "2022-01-07 21:07:00.521267: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: ../tboard/train/plugins/profile/2022_01_07_21_07_00\n",
      "\n",
      "2022-01-07 21:07:00.522487: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for trace.json.gz to ../tboard/train/plugins/profile/2022_01_07_21_07_00/Virajdatts-MacBook-Air.local.trace.json.gz\n",
      "2022-01-07 21:07:00.526632: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: ../tboard/train/plugins/profile/2022_01_07_21_07_00\n",
      "\n",
      "2022-01-07 21:07:00.526884: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for memory_profile.json.gz to ../tboard/train/plugins/profile/2022_01_07_21_07_00/Virajdatts-MacBook-Air.local.memory_profile.json.gz\n",
      "2022-01-07 21:07:00.527287: I tensorflow/core/profiler/rpc/client/capture_profile.cc:251] Creating directory: ../tboard/train/plugins/profile/2022_01_07_21_07_00\n",
      "Dumped tool data for xplane.pb to ../tboard/train/plugins/profile/2022_01_07_21_07_00/Virajdatts-MacBook-Air.local.xplane.pb\n",
      "Dumped tool data for overview_page.pb to ../tboard/train/plugins/profile/2022_01_07_21_07_00/Virajdatts-MacBook-Air.local.overview_page.pb\n",
      "Dumped tool data for input_pipeline.pb to ../tboard/train/plugins/profile/2022_01_07_21_07_00/Virajdatts-MacBook-Air.local.input_pipeline.pb\n",
      "Dumped tool data for tensorflow_stats.pb to ../tboard/train/plugins/profile/2022_01_07_21_07_00/Virajdatts-MacBook-Air.local.tensorflow_stats.pb\n",
      "Dumped tool data for kernel_stats.pb to ../tboard/train/plugins/profile/2022_01_07_21_07_00/Virajdatts-MacBook-Air.local.kernel_stats.pb\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "173/173 [==============================] - ETA: 0s - loss: 0.5326 - accuracy: 0.7651"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-07 21:07:06.232604: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "173/173 [==============================] - 8s 35ms/step - loss: 0.5326 - accuracy: 0.7651 - val_loss: 0.3895 - val_accuracy: 0.8127\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.38945, saving model to ../model/first_transformer.h5\n",
      "Epoch 2/10\n",
      "173/173 [==============================] - 6s 33ms/step - loss: 0.3970 - accuracy: 0.8214 - val_loss: 0.3371 - val_accuracy: 0.8485\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.38945 to 0.33707, saving model to ../model/first_transformer.h5\n",
      "Epoch 3/10\n",
      "173/173 [==============================] - 6s 34ms/step - loss: 0.3496 - accuracy: 0.8419 - val_loss: 0.3286 - val_accuracy: 0.8534\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.33707 to 0.32859, saving model to ../model/first_transformer.h5\n",
      "Epoch 4/10\n",
      "173/173 [==============================] - 6s 34ms/step - loss: 0.3239 - accuracy: 0.8577 - val_loss: 0.3083 - val_accuracy: 0.8697\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.32859 to 0.30827, saving model to ../model/first_transformer.h5\n",
      "Epoch 5/10\n",
      "173/173 [==============================] - 6s 33ms/step - loss: 0.2954 - accuracy: 0.8702 - val_loss: 0.2978 - val_accuracy: 0.8697\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.30827 to 0.29782, saving model to ../model/first_transformer.h5\n",
      "Epoch 6/10\n",
      "173/173 [==============================] - 6s 33ms/step - loss: 0.2869 - accuracy: 0.8796 - val_loss: 0.2737 - val_accuracy: 0.8844\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.29782 to 0.27371, saving model to ../model/first_transformer.h5\n",
      "Epoch 7/10\n",
      "173/173 [==============================] - 6s 33ms/step - loss: 0.2550 - accuracy: 0.8953 - val_loss: 0.2734 - val_accuracy: 0.8893\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.27371 to 0.27342, saving model to ../model/first_transformer.h5\n",
      "Epoch 8/10\n",
      "173/173 [==============================] - 6s 32ms/step - loss: 0.2268 - accuracy: 0.9064 - val_loss: 0.2437 - val_accuracy: 0.9137\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.27342 to 0.24373, saving model to ../model/first_transformer.h5\n",
      "Epoch 9/10\n",
      "173/173 [==============================] - 6s 33ms/step - loss: 0.2104 - accuracy: 0.9154 - val_loss: 0.2532 - val_accuracy: 0.9055\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.24373\n",
      "Epoch 10/10\n",
      "173/173 [==============================] - 6s 33ms/step - loss: 0.1942 - accuracy: 0.9247 - val_loss: 0.2595 - val_accuracy: 0.8941\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.24373\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x16b0fe3d0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MC = tf.keras.callbacks.ModelCheckpoint(\n",
    "    '../model/first_transformer.h5',\n",
    "    monitor='val_loss',\n",
    "    save_best_only='True',\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "ES = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=5,\n",
    "    verbose=1,\n",
    "    restore_best_weights='True'\n",
    ")\n",
    "\n",
    "TB = tf.keras.callbacks.TensorBoard('../tboard/')\n",
    "\n",
    "tmodel.fit(train_padded,\n",
    "              train_labels,\n",
    "               epochs=10,\n",
    "               validation_data=(test_padded, test_labels),\n",
    "               callbacks=[ES, MC, TB]\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c40ad0b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english)\n",
      "Downloading: 100%|███████████████████████████████████████████████████████████████████████| 629/629 [00:00<00:00, 273kB/s]\n",
      "Downloading: 100%|████████████████████████████████████████████████████████████████████| 256M/256M [00:09<00:00, 27.1MB/s]\n",
      "2022-01-07 21:09:32.275431: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "Some layers from the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english were not used when initializing TFDistilBertForSequenceClassification: ['dropout_19']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english and are newly initialized: ['dropout_20']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Downloading: 100%|████████████████████████████████████████████████████████████████████| 48.0/48.0 [00:00<00:00, 15.2kB/s]\n",
      "Downloading: 100%|████████████████████████████████████████████████████████████████████| 226k/226k [00:00<00:00, 2.73MB/s]\n"
     ]
    }
   ],
   "source": [
    "#test_labels\n",
    "\n",
    "hf_sentiment_classifier = pipeline('sentiment-analysis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b250813a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1min 7s ± 975 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "results = hf_sentiment_classifier(list(test_data['Review'].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ee5d2f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from transformers.pipelines.base import KeyDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6318dd64",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset = load_dataset('csv', data_files='../data/uhack_review_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f6287d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.from_pandas(test_data[[\"Review\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "87acb79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment = []\n",
    "for review in test_data['Review'].values:\n",
    "    sentiment.append(hf_sentiment_classifier(review)[0]['label'])\n",
    "    #print(hf_sentiment_classifier(review)[0]['label'])\n",
    "    #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "5823f0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_value = []\n",
    "for sen in sentiment:\n",
    "    if sen == 'POSITIVE':\n",
    "        sent_value.append(1)\n",
    "    else:\n",
    "        sent_value.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "15675466",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Calculate accuracy percentage between two lists\n",
    "def accuracy_metric(actual, predicted):\n",
    " correct = 0\n",
    " for i in range(len(actual)):\n",
    "        if actual[i] == predicted[i]:\n",
    "            correct += 1\n",
    " return correct / float(len(actual)) * 100.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "b52c14b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84.85342019543974"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_metric(sent_value, list(test_data['Polarity']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('base': conda)",
   "language": "python",
   "name": "python397jvsc74a57bd09856851bd649d589f665b7dab82e57037201f90d2f14c285519476a7544379c3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
